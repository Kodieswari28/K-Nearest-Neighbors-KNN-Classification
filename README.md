# ðŸ§  K-Nearest Neighbors (KNN) Classification

## ðŸ“Œ Objective
The goal of this task is to **understand and implement the K-Nearest Neighbors (KNN) algorithm** for classification problems, experiment with different values of **K**, and visualize decision boundaries.

## ðŸ“‚ Dataset
For this task, we used the **Iris dataset** from Scikit-learn.  
The dataset contains:
- **150 samples**
- **4 features**:
  - Sepal Length
  - Sepal Width
  - Petal Length
  - Petal Width
- **3 classes**:
  - Setosa
  - Versicolor
  - Virginica

## ðŸ›  Tools & Libraries
- Python 3.x  
- Pandas  
- NumPy  
- Matplotlib  
- Seaborn  
- Scikit-learn  

## ðŸ“Š Steps Performed
1. **Load the dataset** using `sklearn.datasets.load_iris`.
2. **Normalize** the features using `StandardScaler`.
3. **Split the data** into training and testing sets.
4. **Train the KNN classifier** for different values of **K** (1 to 10).
5. **Evaluate model performance** using:
   - Accuracy Score
   - Confusion Matrix
   - Classification Report
6. **Visualize**:
   - Accuracy vs K plot
   - Confusion Matrix Heatmap
   - Decision Boundaries (using first two features for visualization)


##
